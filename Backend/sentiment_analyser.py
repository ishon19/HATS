# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFBrr_kjNTL-c0Xaw1foLQpCU8glVtkt

# ***Get data***
"""

# import json

# Opening JSON file
# f = open('tweets.json')

# returns JSON object as
# a dictionary
# data = json.load(f)
# data = data["response"]["docs"]
# print(data)

"""# ***Import the libraries***"""

import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
import nltk
import time
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from deep_translator import GoogleTranslator
from langdetect import detect
nltk.download("stopwords")
nltk.download("vader_lexicon")

"""# ***Class to Preprocess the tweets***"""

class Preprocessor:
    def __init__(self):
        self.stop_words_e = set(stopwords.words('english'))
        #self.stop_words_h = set(stopwords.words('hindi'))
        self.stop_words_s = set(stopwords.words('spanish'))
        self.ps = PorterStemmer()

    def tokenizer(self, text):
        """ Implement logic to pre-process & tokenize document text.
            Write the code in such a way that it can be re-used for processing the user's query.
            To be implemented."""
        try:
            # Translate text to english
            if detect(text) != "en":
                text = GoogleTranslator(source='auto', target='en').translate(text=text)
                time.sleep(0.201)
        except:
            text = GoogleTranslator(source='auto', target='en').translate(text=text)
            time.sleep(0.201)
        # 1. Conversion of text in lower case.
        text = text.lower()

        # 2. Removal of special characters.
        text = re.sub('[^a-zA-Z0-9 \n]', '', text)

        # 3. Removal of extra whitespaces.
        text = re.sub(" +", " ", text)

        # 4. Tokenization of text on whitespaces.
        tokens = text.split(" ")

        # 5. Removal of stopwords
        final_tokens = []
        for word in tokens:
            if word not in self.stop_words_e:
                final_tokens.append(word)

        # 6. Perform Porter's stemming
        final_stemmed_tokens = []
        for word in final_tokens:
            word = self.ps.stem(word)
            final_stemmed_tokens.append(word)

        final_text = " ".join(final_stemmed_tokens)

        return final_text


class SentimentAnalyzer:
    def __init__(self, data):
        self.data = data

    def get_sentiment(self):
        """# ***Sentiment Analysis***"""
        p = Preprocessor()
        sid = SentimentIntensityAnalyzer()
        print("[get_sentiment] data: ", self.data)
        for i in range(len(self.data)):
            text_analyse = p.tokenizer(
                self.data[i]["tweet_text"])        
            sentiment = sid.polarity_scores(text_analyse)
            self.data[i]['sentiment_score'] = sentiment['compound']
            if sentiment["compound"] >= 0.05:
                self.data[i]["sentiment"] = "Positive"
            elif sentiment["compound"] > -0.05 and sentiment["compound"] < 0.05:
                self.data[i]["sentiment"] = "Neutral"
            elif sentiment["compound"] <= -0.05:
                self.data[i]["sentiment"] = "Negative"
            sentiment = None
        return self.data
    
    def get_sentiment_counts(self):
        sentiment_counts = {"Positive": 0, "Neutral":0, "Negative": 0}
        p = Preprocessor()
        sid = SentimentIntensityAnalyzer()
        for i in range(len(self.data)):
            text_analyse = p.tokenizer(self.data[i]["tweet_text"])
            sentiment = sid.polarity_scores(text_analyse)
            if sentiment["compound"] >= 0.05:
                sentiment_counts["Positive"] += 1
            elif sentiment["compound"] > -0.05 and sentiment["compound"] < 0.05:
                sentiment_counts["Neutral"] += 1
            elif sentiment["compound"] <= -0.05:
                sentiment_counts["Negative"] += 1
        return sentiment_counts
